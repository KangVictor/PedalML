{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Audio Dataset:  44%|████▍     | 2556/5760 [04:36<07:02,  7.59file/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load metadata\n",
    "metadata_file = \"data/generated/processed_audio_metadata.csv\"\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Initialize lists for dataset\n",
    "X_clean_specs = []\n",
    "X_proc_specs = []\n",
    "Y_labels = []\n",
    "\n",
    "# Mel spectrogram parameters (should match model input requirements)\n",
    "sample_rate = 22050\n",
    "n_mels = 128\n",
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "max_frames = 256  # Fixed time dimension for spectrograms\n",
    "\n",
    "# Function to ensure all spectrograms have a fixed size\n",
    "def fix_spectrogram_shape(S_db, max_frames):\n",
    "    num_frames = S_db.shape[1]\n",
    "    if num_frames < max_frames:\n",
    "        pad_width = max_frames - num_frames\n",
    "        S_db = np.pad(S_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        S_db = S_db[:, :max_frames]\n",
    "    return S_db\n",
    "\n",
    "# Progress bar for loading dataset\n",
    "progress_bar = tqdm(total=len(metadata_df), desc=\"Loading Audio Dataset\", unit=\"file\")\n",
    "\n",
    "# Iterate through metadata to load spectrograms\n",
    "for index, row in metadata_df.iterrows():\n",
    "    clean_file = row[\"clean_file\"]\n",
    "    processed_file = row[\"processed_file\"]\n",
    "\n",
    "    try:\n",
    "        # Load clean and processed audio\n",
    "        y_clean, sr = librosa.load(clean_file, sr=sample_rate)\n",
    "        y_proc, sr = librosa.load(processed_file, sr=sample_rate)\n",
    "\n",
    "        # Compute mel spectrograms\n",
    "        S_clean = librosa.feature.melspectrogram(y=y_clean, sr=sr, \n",
    "                                                 n_mels=n_mels, n_fft=frame_length, hop_length=hop_length)\n",
    "        S_clean_db = librosa.power_to_db(S_clean, ref=np.max)\n",
    "        S_clean_db = fix_spectrogram_shape(S_clean_db, max_frames)[..., np.newaxis]  # Add channel dimension\n",
    "\n",
    "        S_proc = librosa.feature.melspectrogram(y=y_proc, sr=sr, \n",
    "                                                n_mels=n_mels, n_fft=frame_length, hop_length=hop_length)\n",
    "        S_proc_db = librosa.power_to_db(S_proc, ref=np.max)\n",
    "        S_proc_db = fix_spectrogram_shape(S_proc_db, max_frames)[..., np.newaxis]  # Add channel dimension\n",
    "\n",
    "        # Store spectrograms\n",
    "        X_clean_specs.append(S_clean_db)\n",
    "        X_proc_specs.append(S_proc_db)\n",
    "\n",
    "        # Store effect levels as labels\n",
    "        effect_vector = [\n",
    "            row[\"distortion_level\"],\n",
    "            row[\"reverb_level\"],\n",
    "            row[\"chorus_level\"],\n",
    "            row[\"echo_level\"]\n",
    "        ]\n",
    "        Y_labels.append(effect_vector)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {processed_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Convert lists to NumPy arrays for model training\n",
    "X_clean_specs = np.array(X_clean_specs)\n",
    "X_proc_specs = np.array(X_proc_specs)\n",
    "Y_labels = np.array(Y_labels)  # Now contains effect levels as a continuous vector\n",
    "\n",
    "print(f\"Dataset loaded: {X_clean_specs.shape[0]} samples.\")\n",
    "print(f\"X_clean_specs shape: {X_clean_specs.shape}\")\n",
    "print(f\"X_proc_specs shape: {X_proc_specs.shape}\")\n",
    "print(f\"Y_labels shape: {Y_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset for training\n",
    "Xc_train, Xc_val, Xp_train, Xp_val, Y_train, Y_val = train_test_split(\n",
    "    X_clean_specs, X_proc_specs, Y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {Xc_train.shape[0]}, Validation samples: {Xc_val.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ghvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
